{"cells":[{"cell_type":"code","execution_count":1,"id":"7f6ec407","metadata":{},"outputs":[],"source":["start_date = \"2022-10-24\"\n","end_date = \"2022-10-31\"\n","\n","dataset_id = \"transaction_network\"\n","table_name = \"graph_week_10_31\"\n","edge_table_name = table_name + \"_edge\"\n","node_table_name = table_name + \"_node\"\n","sub_node_table_name = node_table_name + \"_sub\"\n","sub_edge_table_name = edge_table_name + \"_sub\"\n","\n","temp_view_name = \"temp_data\"\n","edge_temp_view_name = \"edge_temp_data\""]},{"cell_type":"code","execution_count":null,"id":"c94483f4","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"id":"78df095b","metadata":{},"outputs":[],"source":["import findspark, pyspark, os, sys\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","from pyspark import SparkConf, SparkContext, SQLContext\n","\n","from google.cloud import bigquery\n","client = bigquery.Client()\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from collections import Counter"]},{"cell_type":"code","execution_count":3,"id":"b8253d7d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"]},{"name":"stderr","output_type":"stream","text":["Ivy Default Cache set to: /root/.ivy2/cache\n","The jars for the packages stored in: /root/.ivy2/jars\n","graphframes#graphframes added as a dependency\n",":: resolving dependencies :: org.apache.spark#spark-submit-parent-34719a0d-0ce3-4cfa-a002-4698bd6bf3c7;1.0\n","\tconfs: [default]\n","\tfound graphframes#graphframes;0.8.2-spark3.1-s_2.12 in spark-packages\n","\tfound org.slf4j#slf4j-api;1.7.16 in central\n",":: resolution report :: resolve 221ms :: artifacts dl 6ms\n","\t:: modules in use:\n","\tgraphframes#graphframes;0.8.2-spark3.1-s_2.12 from spark-packages in [default]\n","\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n","\t---------------------------------------------------------------------\n","\t|                  |            modules            ||   artifacts   |\n","\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n","\t---------------------------------------------------------------------\n","\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n","\t---------------------------------------------------------------------\n",":: retrieving :: org.apache.spark#spark-submit-parent-34719a0d-0ce3-4cfa-a002-4698bd6bf3c7\n","\tconfs: [default]\n","\t0 artifacts copied, 2 already retrieved (0kB/7ms)\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/12/16 00:35:20 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/12/16 00:35:20 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/12/16 00:35:20 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/12/16 00:35:20 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","22/12/16 00:35:23 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar added multiple times to distributed cache.\n","22/12/16 00:35:23 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar added multiple times to distributed cache.\n"]}],"source":["SUBMIT_ARGS = \"--packages graphframes:graphframes:0.8.2-spark3.1-s_2.12 pyspark-shell\"\n","os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n","\n","conf = SparkConf().setAll([('spark.jars', 'gs://spark-lib/bigquery/spark-3.1-bigquery-0.27.1-preview.jar')])\n","sc = SparkContext(conf=conf)\n","\n","pyfiles = str(sc.getConf().get(u'spark.submit.pyFiles')).split(',')\n","sys.path.extend(pyfiles)\n","\n","sqlContext = SQLContext(sparkContext=sc)\n","spark = sqlContext.sparkSession\n","\n","bucket = \"bd6893_data_yq\"\n","spark.conf.set('temporaryGcsBucket', bucket)"]},{"cell_type":"code","execution_count":4,"id":"0c3c96ac","metadata":{},"outputs":[],"source":["# Prepare a reference to a new dataset for storing the query results.\n","dataset_id_full = f\"{client.project}.{dataset_id}\"\n","dataset = bigquery.Dataset(dataset_id_full)\n","\n","# # Create the new BigQuery dataset.\n","# dataset = client.create_dataset(dataset)\n","\n","# Configure the query job.\n","job_config = bigquery.QueryJobConfig()\n","job_config.destination = f\"{dataset_id_full}.{table_name}\"\n","\n","# Execute the query\n","post_merge_query = f\"\"\"\n","    SELECT * FROM big-data-6893-yunjie-qian.eth.transactions\n","    WHERE DATE(block_timestamp) >= \"{start_date}\" AND DATE(block_timestamp) < \"{end_date}\"\n","    AND (to_address) IS NOT NULL\n","    AND (gas_price) IS NOT NULL\n","\"\"\"\n","# post_merge = client.query(post_merge_query, job_config=job_config)\n","# post_merge.result()"]},{"cell_type":"code","execution_count":5,"id":"d77d684d","metadata":{},"outputs":[],"source":["temp_data = spark.read.format('bigquery') \\\n","    .option('table', f'big-data-6893-yunjie-qian:{dataset_id}.{table_name}') \\\n","    .load()\n","\n","temp_data.createOrReplaceTempView(temp_view_name)\n","# temp_data.show(3)"]},{"cell_type":"code","execution_count":null,"id":"f26f829c","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"id":"3166a4e7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- src: string (nullable = true)\n"," |-- dst: string (nullable = true)\n"," |-- total_value: decimal(38,9) (nullable = true)\n"," |-- min_gas_price: long (nullable = true)\n"," |-- transaction_count: long (nullable = false)\n","\n"]}],"source":["edge_query = f'''\n","SELECT from_address AS src, to_address AS dst,\n","SUM(value) AS total_value, MIN(gas_price) AS min_gas_price, COUNT(input) AS transaction_count\n","FROM {temp_view_name} \n","GROUP BY from_address, to_address\n","'''\n","edge_df = spark.sql(edge_query)\n","edge_df.createOrReplaceTempView(edge_temp_view_name)\n","# edge_df.show(3)\n","edge_df.printSchema()"]},{"cell_type":"code","execution_count":7,"id":"b4a8799a","metadata":{},"outputs":[],"source":["# edge_df.write.format('bigquery') \\\n","#   .option('table', f'{dataset_id}.{edge_table_name}') \\\n","#   .save()"]},{"cell_type":"code","execution_count":null,"id":"fb1b9a73","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"id":"ecc60baf","metadata":{},"outputs":[],"source":["node_query_src = f'''\n","SELECT src AS id, COUNT(src) AS outdegree, \n","SUM(total_value) AS out_total_value, SUM(transaction_count) AS out_total_transaction\n","FROM {edge_temp_view_name}\n","GROUP BY src\n","'''\n","node_df_src = spark.sql(node_query_src)\n","\n","node_query_dst = f'''\n","SELECT dst AS id, COUNT(dst) AS indegree, \n","SUM(total_value) AS in_total_value, SUM(transaction_count) AS in_total_transaction\n","FROM {edge_temp_view_name}\n","GROUP BY dst\n","'''\n","node_df_dst = spark.sql(node_query_dst)"]},{"cell_type":"code","execution_count":9,"id":"c99d9287","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- id: string (nullable = true)\n"," |-- outdegree: long (nullable = true)\n"," |-- out_total_value: decimal(38,9) (nullable = true)\n"," |-- out_total_transaction: long (nullable = true)\n"," |-- indegree: long (nullable = true)\n"," |-- in_total_value: decimal(38,9) (nullable = true)\n"," |-- in_total_transaction: long (nullable = true)\n"," |-- degree: long (nullable = true)\n","\n"]}],"source":["node_df = node_df_src.join(node_df_dst, on=\"id\", how=\"full\")\n","node_df = node_df.na.fill(value=0)\n","node_df = node_df.withColumn('degree', node_df.indegree + node_df.outdegree)\n","# node_df.show(3)\n","node_df.printSchema()"]},{"cell_type":"code","execution_count":10,"id":"8c82337c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["node_df.write.format('bigquery') \\\n","  .option('table', f'{dataset_id}.{node_table_name}') \\\n","  .save()"]},{"cell_type":"code","execution_count":null,"id":"f196e213","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"9275d359","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"id":"65000757","metadata":{},"outputs":[],"source":["from graphframes import *\n","\n","g = GraphFrame(node_df, edge_df)"]},{"cell_type":"code","execution_count":12,"id":"83e86ced","metadata":{},"outputs":[],"source":["subg = g.filterVertices(\"degree >= 30\").filterEdges(\"transaction_count >= 10\").dropIsolatedVertices()"]},{"cell_type":"code","execution_count":13,"id":"99e57efd","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["subg.vertices.write.format('bigquery') \\\n","  .option('table', f'{dataset_id}.{sub_node_table_name}') \\\n","  .save()\n","\n","subg.edges.write.format('bigquery') \\\n","  .option('table', f'{dataset_id}.{sub_edge_table_name}') \\\n","  .save()"]},{"cell_type":"code","execution_count":null,"id":"710109ec","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ff66cbea","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"id":"375d2259","metadata":{},"outputs":[],"source":["# from pyspark.sql.functions import countDistinct\n","\n","# labelprop = g.labelPropagation(maxIter=3)\n","# labelprop.select(countDistinct(\"label\")).show()"]},{"cell_type":"code","execution_count":15,"id":"8389c911","metadata":{},"outputs":[],"source":["# sc.setCheckpointDir(dirName=\"/home/big-data-6893-yunjie-qian/graphframes_cps\")\n","\n","# scc = g.connectedComponents()\n","# scc.show(10)\n","# result.select(\"id\", \"component\").orderBy(\"component\").show()\n","\n","# scc.select(\"id\", \"component\").show()\n","# .groupBy('component').count().orderBy('count', ascending=False).show()"]},{"cell_type":"code","execution_count":null,"id":"d02471e1","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0b719a43","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":16,"id":"0923a502","metadata":{},"outputs":[],"source":["# # Create a Vertex DataFrame with unique ID column \"id\"\n","# v = sqlContext.createDataFrame([\n","#   (\"a\", \"Alice\", 34),\n","#   (\"b\", \"Bob\", 36),\n","#   (\"c\", \"Charlie\", 30),\n","# ], [\"id\", \"name\", \"age\"])\n","\n","# # Create an Edge DataFrame with \"src\" and \"dst\" columns\n","# e = sqlContext.createDataFrame([\n","#   (\"a\", \"b\", \"friend\"),\n","#   (\"b\", \"c\", \"follow\"),\n","#   (\"c\", \"b\", \"follow\"),\n","# ], [\"src\", \"dst\", \"relationship\"])\n","\n","# # Create a GraphFrame\n","# g = GraphFrame(v, e)\n","\n","# # Query: Get in-degree of each vertex.\n","# g.inDegrees.show()\n","\n","# # Query: Count the number of \"follow\" connections in the graph.\n","# g.edges.filter(\"relationship = 'follow'\").count()\n","\n","# # # Run PageRank algorithm, and show results.\n","# # results = g.pageRank(resetProbability=0.01, maxIter=20)\n","# # results.vertices.select(\"id\", \"pagerank\").show()"]},{"cell_type":"code","execution_count":17,"id":"0e05ef71","metadata":{},"outputs":[],"source":["# # from pyspark import SparkContext\n","# # from pyspark.sql import SQLContext\n","# from pyspark.sql import functions\n","# # from graphframes import *\n","# from pyspark.sql.functions import explode\n","\n","# # sc=SparkContext(\"local\", \"degree.py\")\n","# # sqlContext = SQLContext(sc)\n","\n","# def closeness(g):\n","    \n","#     # Get list of vertices. We'll generate all the shortest paths at once using this list.\n","#     # YOUR CODE HERE\n","#     shortestPaths = g.shortestPaths(landmarks = g.vertices.rdd.map(lambda x: x.id).collect())\n","#     # first get all the path lengths.\n","#     pathLength = shortestPaths.select('id', explode('distances'))\n","#     # Break up the map and group by ID for summing\n","#     groupedKey = pathLength.groupBy('key')\n","#     # Sum by ID\n","#     sumOfGroupedDistances = groupedKey.agg(functions.sum('value').alias('c'))\n","#     # Get the inverses and generate desired dataframe.\n","#     return sumOfGroupedDistances.selectExpr('key as id','1/c as closeness')\n","\n","# print(\"Reading in graph for problem 2.\")\n","# graph = sc.parallelize([('A','B'),('A','C'),('A','D'),\n","#     ('B','A'),('B','C'),('B','D'),('B','E'),\n","#     ('C','A'),('C','B'),('C','D'),('C','F'),('C','H'),\n","#     ('D','A'),('D','B'),('D','C'),('D','E'),('D','F'),('D','G'),\n","#     ('E','B'),('E','D'),('E','F'),('E','G'),\n","#     ('F','C'),('F','D'),('F','E'),('F','G'),('F','H'),\n","#     ('G','D'),('G','E'),('G','F'),\n","#     ('H','C'),('H','F'),('H','I'),\n","#     ('I','H'),('I','J'),\n","#     ('J','I')])\n","    \n","# e = sqlContext.createDataFrame(graph,['src','dst'])\n","# v = e.selectExpr('src as id').unionAll(e.selectExpr('dst as id')).distinct()\n","# print(\"Generating GraphFrame.\")\n","# g1 = GraphFrame(v,e)\n","\n","# print(\"Calculating closeness.\")\n","# closeness(g1).sort('closeness',ascending=False).show()"]},{"cell_type":"code","execution_count":18,"id":"bf72c65e","metadata":{},"outputs":[],"source":["# def closeness(g):\n","    \n","#     # Get list of vertices. We'll generate all the shortest paths at once using this list.\n","#     shortestPaths = g.shortestPaths(landmarks = g.vertices.rdd.map(lambda x: x.id).collect())\n","#     # first get all the path lengths.\n","#     pathLength = shortestPaths.select('id', explode('distances'))\n","#     # Break up the map and group by ID for summing\n","#     groupedKey = pathLength.groupBy('key')\n","#     # Sum by ID\n","#     sumOfGroupedDistances = groupedKey.agg(functions.sum('value').alias('c'))\n","#     # Get the inverses and generate desired dataframe.\n","#     return sumOfGroupedDistances.selectExpr('key as id','1/c as closeness')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}