{"cells":[{"cell_type":"code","execution_count":1,"id":"7f6ec407","metadata":{},"outputs":[],"source":["# Manage parameters, ex. date, table name ...\n","\n","start_date = \"2022-12-14\"\n","end_date = \"2022-12-21\"\n","\n","dataset_id = \"transaction_network\"\n","table_name = \"graph_week_12_21\"\n","edge_table_name = table_name + \"_edge\"\n","node_table_name = table_name + \"_node\"\n","sub_node_table_name = node_table_name + \"_sub\"\n","sub_edge_table_name = edge_table_name + \"_sub\"\n","\n","temp_view_name = \"temp_data\"\n","edge_temp_view_name = \"edge_temp_data\""]},{"cell_type":"code","execution_count":2,"id":"78df095b","metadata":{},"outputs":[],"source":["# Connect to BigQuery\n","from google.cloud import bigquery\n","client = bigquery.Client()\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from collections import Counter"]},{"cell_type":"code","execution_count":3,"id":"b8253d7d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"]},{"name":"stderr","output_type":"stream","text":["Ivy Default Cache set to: /root/.ivy2/cache\n","The jars for the packages stored in: /root/.ivy2/jars\n","graphframes#graphframes added as a dependency\n",":: resolving dependencies :: org.apache.spark#spark-submit-parent-925083e0-7798-46f0-982c-b90494bcd7be;1.0\n","\tconfs: [default]\n","\tfound graphframes#graphframes;0.8.2-spark3.1-s_2.12 in spark-packages\n","\tfound org.slf4j#slf4j-api;1.7.16 in central\n",":: resolution report :: resolve 182ms :: artifacts dl 4ms\n","\t:: modules in use:\n","\tgraphframes#graphframes;0.8.2-spark3.1-s_2.12 from spark-packages in [default]\n","\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n","\t---------------------------------------------------------------------\n","\t|                  |            modules            ||   artifacts   |\n","\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n","\t---------------------------------------------------------------------\n","\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n","\t---------------------------------------------------------------------\n",":: retrieving :: org.apache.spark#spark-submit-parent-925083e0-7798-46f0-982c-b90494bcd7be\n","\tconfs: [default]\n","\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/12/22 02:52:31 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/12/22 02:52:31 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/12/22 02:52:31 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/12/22 02:52:31 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","22/12/22 02:52:33 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar added multiple times to distributed cache.\n","22/12/22 02:52:33 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar added multiple times to distributed cache.\n"]}],"source":["# Configure Spark and GraphFrames\n","\n","import findspark, pyspark, os, sys\n","findspark.init()\n","from pyspark.sql import SparkSession\n","from pyspark import SparkConf, SparkContext, SQLContext\n","\n","SUBMIT_ARGS = \"--packages graphframes:graphframes:0.8.2-spark3.1-s_2.12 pyspark-shell\"\n","os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n","\n","conf = SparkConf().setAll([('spark.jars', 'gs://spark-lib/bigquery/spark-3.1-bigquery-0.27.1-preview.jar')])\n","sc = SparkContext(conf=conf)\n","\n","pyfiles = str(sc.getConf().get(u'spark.submit.pyFiles')).split(',')\n","sys.path.extend(pyfiles)\n","\n","sqlContext = SQLContext(sparkContext=sc)\n","spark = sqlContext.sparkSession\n","\n","bucket = \"bd6893_data_yq\"\n","spark.conf.set('temporaryGcsBucket', bucket)"]},{"cell_type":"code","execution_count":4,"id":"0c3c96ac","metadata":{},"outputs":[{"data":{"text/plain":["<google.cloud.bigquery.table.RowIterator at 0x7ff292feaf10>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Prepare a reference to a new dataset for storing the query results.\n","dataset_id_full = f\"{client.project}.{dataset_id}\"\n","dataset = bigquery.Dataset(dataset_id_full)\n","\n","# Create the new BigQuery dataset.\n","dataset = client.create_dataset(dataset)\n","\n","# Configure the query job.\n","job_config = bigquery.QueryJobConfig()\n","job_config.destination = f\"{dataset_id_full}.{table_name}\"\n","\n","# Execute the query\n","post_merge_query = f\"\"\"\n","    SELECT * FROM bigquery-public-data.crypto_ethereum.transactions\n","    WHERE DATE(block_timestamp) >= \"{start_date}\" AND DATE(block_timestamp) < \"{end_date}\"\n","    AND (to_address) IS NOT NULL\n","    AND (gas_price) IS NOT NULL\n","\"\"\"\n","post_merge = client.query(post_merge_query, job_config=job_config)\n","post_merge.result()"]},{"cell_type":"code","execution_count":5,"id":"d77d684d","metadata":{},"outputs":[],"source":["# Get query data, and create view\n","\n","temp_data = spark.read.format('bigquery') \\\n","    .option('table', f'big-data-6893-yunjie-qian:{dataset_id}.{table_name}') \\\n","    .load()\n","\n","temp_data.createOrReplaceTempView(temp_view_name)"]},{"cell_type":"code","execution_count":null,"id":"f26f829c","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"id":"3166a4e7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- src: string (nullable = true)\n"," |-- dst: string (nullable = true)\n"," |-- total_value: decimal(38,9) (nullable = true)\n"," |-- min_gas_price: long (nullable = true)\n"," |-- transaction_count: long (nullable = false)\n","\n"]}],"source":["# Aggregate edge attributes\n","\n","edge_query = f'''\n","SELECT from_address AS src, to_address AS dst,\n","SUM(value) AS total_value, MIN(gas_price) AS min_gas_price, COUNT(input) AS transaction_count\n","FROM {temp_view_name} \n","GROUP BY from_address, to_address\n","'''\n","edge_df = spark.sql(edge_query)\n","edge_df.createOrReplaceTempView(edge_temp_view_name)\n","# edge_df.show(3)\n","edge_df.printSchema()"]},{"cell_type":"code","execution_count":7,"id":"b4a8799a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Write edge data to BigQuery table\n","\n","edge_df.write.format('bigquery') \\\n","  .option('table', f'{dataset_id}.{edge_table_name}') \\\n","  .save()"]},{"cell_type":"code","execution_count":null,"id":"fb1b9a73","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"id":"ecc60baf","metadata":{},"outputs":[],"source":["# Aggregate node degrees and other node attributes\n","\n","node_query_src = f'''\n","SELECT src AS id, COUNT(src) AS outdegree, \n","SUM(total_value) AS out_total_value, SUM(transaction_count) AS out_total_transaction\n","FROM {edge_temp_view_name}\n","GROUP BY src\n","'''\n","node_df_src = spark.sql(node_query_src)\n","\n","node_query_dst = f'''\n","SELECT dst AS id, COUNT(dst) AS indegree, \n","SUM(total_value) AS in_total_value, SUM(transaction_count) AS in_total_transaction\n","FROM {edge_temp_view_name}\n","GROUP BY dst\n","'''\n","node_df_dst = spark.sql(node_query_dst)"]},{"cell_type":"code","execution_count":9,"id":"c99d9287","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- id: string (nullable = true)\n"," |-- outdegree: long (nullable = true)\n"," |-- out_total_value: decimal(38,9) (nullable = true)\n"," |-- out_total_transaction: long (nullable = true)\n"," |-- indegree: long (nullable = true)\n"," |-- in_total_value: decimal(38,9) (nullable = true)\n"," |-- in_total_transaction: long (nullable = true)\n"," |-- degree: long (nullable = true)\n","\n"]}],"source":["# Preprocess node data\n","\n","node_df = node_df_src.join(node_df_dst, on=\"id\", how=\"full\")\n","node_df = node_df.na.fill(value=0)\n","node_df = node_df.withColumn('degree', node_df.indegree + node_df.outdegree)\n","# node_df.show(3)\n","node_df.printSchema()"]},{"cell_type":"code","execution_count":10,"id":"8c82337c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Write node data to BigQuery table\n","\n","node_df.write.format('bigquery') \\\n","  .option('table', f'{dataset_id}.{node_table_name}') \\\n","  .save()"]},{"cell_type":"code","execution_count":null,"id":"9275d359","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"id":"65000757","metadata":{},"outputs":[],"source":["# Create Graph\n","\n","from graphframes import *\n","g = GraphFrame(node_df, edge_df)"]},{"cell_type":"code","execution_count":12,"id":"83e86ced","metadata":{},"outputs":[],"source":["# Filter to get a subgraph\n","\n","subg = g.filterVertices(\"degree >= 30\").filterEdges(\"transaction_count >= 10\").dropIsolatedVertices()"]},{"cell_type":"code","execution_count":13,"id":"99e57efd","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Write the subgraph edges and nodes to BigQuery\n","\n","subg.vertices.write.format('bigquery') \\\n","  .option('table', f'{dataset_id}.{sub_node_table_name}') \\\n","  .save()\n","\n","subg.edges.write.format('bigquery') \\\n","  .option('table', f'{dataset_id}.{sub_edge_table_name}') \\\n","  .save()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}
